{
  "title": "Analyzing GPT-4 Tokens",
  "publishedAt": "2024-05-02",
  "summary": "For this article I used Llama3 to analyze GPT-4 tokens, revealing a strong bias towards English and code.",
  "tags": [
    "article",
    "data visualization",
    "AI"
  ],
  "tagsAsSlugs": [
    "article",
    "data-visualization",
    "ai"
  ],
  "slug": "analyzing-gpt-4-tokens",
  "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var k=Object.getPrototypeOf,N=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var s in e)o(a,s,{get:e[s],enumerable:!0})},r=(a,e,s,l)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of u(e))!N.call(a,t)&&t!==s&&o(a,t,{get:()=>e[t],enumerable:!(l=m(e,t))||l.enumerable});return a};var f=(a,e,s)=>(s=a!=null?h(k(a)):{},r(e||!a||!a.__esModule?o(s,\"default\",{value:a,enumerable:!0}):s,a)),b=a=>r(o({},\"__esModule\",{value:!0}),a);var i=g((I,c)=>{c.exports=_jsx_runtime});var T={};y(T,{default:()=>d,frontmatter:()=>w});var n=f(i()),w={title:\"Analyzing GPT-4 Tokens\",publishedAt:\"2024-05-02\",summary:\"For this article I used Llama3 to analyze GPT-4 tokens, revealing a strong bias towards English and code.\",tags:[\"article\",\"data visualization\",\"AI\"]};function p(a){let e={a:\"a\",blockquote:\"blockquote\",code:\"code\",h2:\"h2\",hr:\"hr\",li:\"li\",p:\"p\",pre:\"pre\",span:\"span\",ul:\"ul\",...a.components},{Image:s}=e;return s||v(\"Image\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"I recently watched a YouTube video by Andrej Karpathy explaining the quirky behavior of LLMs through their tokenizers. He addresses questions such as why LLMs struggle with spelling or react unpredictably to specific terms like SolidGoldMagikKarp. If you haven't seen \",(0,n.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=zduSFxRajkE&t=4724s\",children:\"this video\"}),\" yet, I highly recommend it.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Karpathy explains that tokens are akin to the atoms of large language models, shaping how they perceive and interact with text. This inherent structure not only facilitates but also limits their capabilities. He notes that the tokens for OpenAI models, like GPT-4, are publicly accessible. This caught my attention and I decided to examine these tokens using Llama3, a capable and freely available large language model that I could run on my own laptop.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"method\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#method\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Method\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The tokens for GPT-4 are accessible through a \",(0,n.jsx)(e.a,{href:\"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\",children:\"link\"}),\" found in OpenAI's \",(0,n.jsx)(e.a,{href:\"https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L73\",children:\"GitHub repository\"}),\" for the TikToken. These tokens are encoded in Base64, requiring decoding to reveal a \",(0,n.jsx)(e.a,{href:\"https://github.com/vnglst/gpt4-tokens/blob/main/cl100k_base.txt\",children:\"list of approximately 100,000 tokens\"}),\". The initial entries mostly consist of single or dual characters. As the list progresses, it begins to include full words and fragments of words.\"]}),`\n`,(0,n.jsx)(e.pre,{className:\"language-txt\",children:(0,n.jsxs)(e.code,{className:\"language-txt code-highlight\",children:[(0,n.jsx)(e.span,{className:\"code-line\",children:` available\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`mt\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` Bl\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` ...\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` block\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`Input\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` keep\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`Count\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`open\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` ['\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` throw\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`uilder\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`Action\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` things\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:`True\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` url\n`}),(0,n.jsx)(e.span,{className:\"code-line\",children:` Bo\n`})]})}),`\n`,(0,n.jsx)(e.p,{children:\"This list includes fragments that resemble syntax elements or identifiers from various programming languages. My goal was to categorize all the tokens using Llama3 into the following categories:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Natural languages\",`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"English (en)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Spanish (es)\"}),`\n`,(0,n.jsx)(e.li,{children:\"German (de)\"}),`\n`,(0,n.jsx)(e.li,{children:\"etc.\"}),`\n`]}),`\n`]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Computer languages\",`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Java\"}),`\n`,(0,n.jsx)(e.li,{children:\"C#\"}),`\n`,(0,n.jsx)(e.li,{children:\"Python\"}),`\n`,(0,n.jsx)(e.li,{children:\"etc.\"}),`\n`]}),`\n`]}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"prompting\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prompting\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prompting\"]}),`\n`,(0,n.jsx)(e.p,{children:\"After some trial and error, I created the following prompt for Llama3:\"}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"You are a token categorizer. You categorize lists of tokens into types: code, lang, unknown. The types have the following definitions:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:'code: tokens that are part of a programming language (such as Python, JavaScript abbreviated as \"js\", C#, etc.)'}),`\n`,(0,n.jsx)(e.li,{children:\"lang: tokens that are part of some natural language.\"}),`\n`,(0,n.jsx)(e.li,{children:\"unknown: are single characters or tokens where the origin cannot be assessed.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"The next categorization is lang. This specifies which language this is part of. The corresponding languages are:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:'for type code: \"javascript\", \"c#\", \"python\", \"java\", \"unknown\", etc.'}),`\n`,(0,n.jsx)(e.li,{children:'for type lang: \"en\", \"es\", \"de\", \"nl\", \"unknown\", etc.'}),`\n`,(0,n.jsx)(e.li,{children:\"for type unknown: leave this empty.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:`Also, add a property called definition to the objects. It should contain a short description of the token. It should not be longer than a few words.\nThe results should be presented in a JSON structure that clearly defines categories for each token. Here\\u2019s an example of how to structure this JSON:`}),`\n`,(0,n.jsx)(e.p,{children:\"Input:\"}),`\n`,(0,n.jsx)(e.p,{children:`liability\nbeam\nNotFound\nCharles\n.SequentialGroup\n\\u043E\\u043B\\u044C\\u043A\\u043E\n_person\n.history\nTextView\n__\n\\xEDs\nMarkt\nonDataChange\nphotoshop`}),`\n`,(0,n.jsx)(e.p,{children:\"Output:\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{className:\"language-json\",children:(0,n.jsxs)(e.code,{className:\"language-json code-highlight\",children:[(0,n.jsxs)(e.span,{className:\"code-line\",children:[(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"liability\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"en\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Legal responsibility\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"beam\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"en\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Line of light\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"NotFound\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"code\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"javascript\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Error message\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"Charles\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"en\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Name\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\".SequentialGroup\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"code\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"java\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Linear processing\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"\\u043E\\u043B\\u044C\\u043A\\u043E\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"ru\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Only\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"_person\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"code\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"python\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Individual\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\".history\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"code\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"javascript\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Past events\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"TextView\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"code\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"java\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Text display\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"__\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"unknown\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"\\xEDs\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"unknown\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"Markt\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"de\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Market or fair\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"onDataChange\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"code\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"unknown\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Data update\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[\"    \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"photoshop\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"{\"}),(0,n.jsx)(e.span,{className:\"token property\",children:'\"type\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"lang\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"en\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,n.jsx)(e.span,{className:\"token property\",children:'\"definition\"'}),(0,n.jsx)(e.span,{className:\"token operator\",children:\":\"}),\" \",(0,n.jsx)(e.span,{className:\"token string\",children:'\"Image editing software\"'}),(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),`\n`]}),(0,n.jsxs)(e.span,{className:\"code-line\",children:[(0,n.jsx)(e.span,{className:\"token punctuation\",children:\"}\"}),`\n`]})]})}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"Do a linguistic analysis of each token. Respond only with valid JSON. Do not add backticks.\"}),`\n`]}),`\n`,(0,n.jsx)(e.hr,{}),`\n`,(0,n.jsxs)(e.p,{children:[\"I stored the JSON result of this prompt in a SQLite database to facilitate easier analysis of the different categories. I used a \",(0,n.jsx)(e.a,{href:\"https://github.com/vnglst/gpt4-tokens/blob/main/analyse.py\",children:\"Python script\"}),\" and the Ollama endpoints for this.\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"results\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#results\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Results\"]}),`\n`,(0,n.jsx)(e.p,{children:\"After a few nights of hard work, Llama3 had processed about 97% of the tokens, and the results are illuminating.\"}),`\n`,(0,n.jsx)(s,{src:\"/static/images/categories.png\",width:589,height:495}),`\n`,(0,n.jsxs)(e.p,{children:[\"As you can see, the majority of GPT-4's tokens are related to coding. The \",(0,n.jsx)(e.code,{children:\"unknown\"}),\" category includes single characters or groups of characters that could not be clearly categorized.\"]}),`\n`,(0,n.jsx)(s,{src:\"/static/images/natural-languages.png\",width:589,height:495}),`\n`,(0,n.jsx)(e.p,{children:\"Focusing on natural language tokens, a significant majority are identified as English words or fragments. Spanish, the second most represented language, has only 1064 tokens.\"}),`\n`,(0,n.jsx)(s,{src:\"/static/images/programming-languages.png\",width:589,height:495}),`\n`,(0,n.jsxs)(e.p,{children:[\"The analysis of programming languages shows Java and JavaScript leading. However, categorizing syntax elements is complex. For example, the token \",(0,n.jsx)(e.code,{children:\"getFullYear\"}),\" is classified under Java but could also belong to JavaScript. So I would not read too much into this chart.\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"conclusions\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusions\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusions\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The reliability of Llama3's categorization is questionable, not due to its limitations but because of the inherent challenge in accurately categorizing small word or code fragments. Nevertheless, this analysis provides intriguing insights into the possible biases in GPT-4.\"}),`\n`,(0,n.jsx)(e.p,{children:\"It is evident that GPT-4 has a stronger focus on English than I anticipated. Of course the main language of the internet is English, but I only counted 124 tokens that could be classified as Dutch words. This could partially explain why GPT-4 underperforms in my native language\"}),`\n`,(0,n.jsx)(e.p,{children:\"Surprisingly, the tokens are more code-centric than I expected. While I assumed a greater emphasis on natural language tokens across various languages, the data suggests a reliance on code, at least in training the tokenizer.\"}),`\n`,(0,n.jsx)(e.p,{children:\"I hope you found this post interesting. This was my first attempt at using a large language model for a categorization task. The implementation scripts are straightforward, but I had to invest more in error handling and recovery so that the analysis would keep running even when a prompt failed to deliver a parsable result.\"})]})}function d(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,{...a,children:(0,n.jsx)(p,{...a})}):p(a)}function v(a,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+a+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return b(T);})();\n;return Component;",
  "readingTime": {
    "text": "5 min read",
    "minutes": 4.73,
    "time": 283800,
    "words": 946
  }
}